{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: The Maximum Entropy Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ Demonstrate the maximum entropy principle through some examples\n",
    "\n",
    "## How do we come up with the right probability assignments?\n",
    "\n",
    "In applications we often found ourselves in a situation where we have to pick the probabilities of a given variable.\n",
    "An important question is how we come up with these probabilities.\n",
    "Is there a systematic theoretical framework we could follow?\n",
    "There are basically three widely accepted ways:\n",
    "\n",
    "+ The principle of insufficient reason.\n",
    "+ The principle of maximum entropy.\n",
    "+ The principle of transformation groups.\n",
    "We will briefly introduce the first two.\n",
    "\n",
    "### Principle of insufficient reason\n",
    "> The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favorable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possibleâ€¦\n",
    "Pierre-Simon Laplace\n",
    "\n",
    "Assume that the random variable $X$ can take $N$ possible values, $1, 2,\\dots,N$.\n",
    "If this is all we know about this random variable then *the principle of insufficient reason* tells us to set:\n",
    "$$\n",
    "p(x) = \\frac{1}{N},\n",
    "$$\n",
    "for $x$ in $\\{1,2,\\dots,N\\}$.\n",
    "That is, the principle of insufficient reason tells us to assign the same probability to each possibility.\n",
    "Intuitively, any other choice we could make would introduce a bias towards one value or another.\n",
    "\n",
    "### Example: Throwing the die\n",
    "Consider a six-sided die with sides numbered $1$ to $6$.\n",
    "Call $X$ the random variable corresponding to an experiment of throwing the die.\n",
    "What is the probability of the die taking a specific value.\n",
    "Using the principle of insufficient reason, we set:\n",
    "$$\n",
    "p(X=x) = \\frac{1}{6}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy of 2-state Distribution\n",
    "\n",
    "Consdider a distribution with two states, say $0$ and $1$.\n",
    "If the probability of $0$ is $p$, then the entropy of the distribution is:\n",
    "$$\n",
    "H_2(p, 1-p) = -p\\log p - (1-p)\\log (1-p)\n",
    "$$\n",
    "Let's plot this with respect to p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division, with_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "eps = 1e-8\n",
    "p = np.linspace(eps, 1. - eps, 100)\n",
    "H = -p * np.log(p) - (1. - p) * np.log(1. - p)\n",
    "ax.plot(p, H)\n",
    "ax.set_xlabel('$p$')\n",
    "ax.set_ylabel('$H_2(p, 1-p)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Brandeis Dice Problem\n",
    "This is from the 1962 Brandeis lectures of E. T. Jaynes.\n",
    "\n",
    "> When a die is tossed, the number of spots up can have any value $i$ in $1\\le i \\le 6$. Suppose a die has been tossed $N$ times and we are told only that the average number of spots up was not $3.5$ as we might expect from an \"honest\" but 4.5. Given this information, <u>and nothing else</u>, what probability should we assign to $i$ spots on the next toss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data impose the following mean value constraint:\n",
    "$$\n",
    "\\sum_{i=1}^6 i p_i = 4.5.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partition function is:\n",
    "$$\n",
    "Z(\\lambda) = \\sum_{i}e^{-\\lambda i} = e^{-\\lambda} + e^{-2\\lambda} + \\dots + e^{-6\\lambda},\n",
    "$$\n",
    "or\n",
    "$$\n",
    "Z(\\lambda) = \\left(e^{-\\lambda}\\right)^1+\\left(e^{-\\lambda}\\right)^2 + \\dots + \\left(e^{-\\lambda}\\right)^6,\n",
    "$$\n",
    "which is equal to:\n",
    "$$\n",
    "Z(\\lambda) = \\frac{e^{-\\lambda}\\left(1-e^{-6\\lambda}\\right)}{1 - e^{-\\lambda}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find $\\lambda$, we must solve:\n",
    "$$\n",
    "-\\frac{\\partial Z}{\\partial \\lambda} = 4.5.\n",
    "$$\n",
    "This becomes:\n",
    "$$\n",
    "\\frac{1 - 7e^{-6\\lambda} + 6e^{7\\lambda}}{(1 - e^{-\\lambda})(1 - e^{-6\\lambda})} = 4.5,\n",
    "$$\n",
    "or\n",
    "$$\n",
    "3\\left(e^{-\\lambda}\\right)^7 - 5 \\left(e^{-\\lambda}\\right)^6 + 9e^{-\\lambda} - 7 = 0.\n",
    "$$\n",
    "Let's solve this numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# x = exp(-lambda)\n",
    "def f(x):\n",
    "    return 3. * x ** 7 - 5 * x ** 6 + 9. * x - 7.\n",
    "\n",
    "# Left bound for x\n",
    "a = 0.\n",
    "# Right bound for x\n",
    "b = 10.\n",
    "x = brentq(f, a, b)\n",
    "lam = -math.log(x)\n",
    "print('Lambda:', lam)\n",
    "# Evaluate the partition function at this lambda\n",
    "Z = math.exp(-lam) * (1. - math.exp(-6 * lam)) / (1. - math.exp(-lam))\n",
    "print('Z:', Z)\n",
    "# The log of Z\n",
    "log_Z = math.log(Z)\n",
    "print('log Z:', log_Z)\n",
    "# The maximum entropy probabilities\n",
    "p = np.exp(-lam * np.arange(1, 7)) / Z\n",
    "print('p: ', p) \n",
    "# The entropty of this distribution is:\n",
    "S = log_Z + lam * 4.5\n",
    "print('S:', S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot this:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.bar(np.arange(1, 7), p, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "+ Repeat the analysis for a mean of 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Repeat the analysis for a mean of $1.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Repeat assuming that we now know that the second moment is 2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
